import pandas as pd
import json
import os
import asyncio
import httpx
from datetime import datetime
from bs4 import BeautifulSoup
import re
from tenacity import retry, stop_after_attempt, wait_exponential
from more_itertools import chunked
import time
import subprocess
from sys import argv, executable
# Solutuion:
# 1.L∆∞u m·ªói file v·ªõi 1000 ids, v√† ph·∫ßn c√≤n l·∫°i v√†o file cu·ªëi
# 2.Retry ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªè s√≥t
# 3.Check point ƒë·ªÉ ch·∫°y ti·∫øp t·ª•c khi b·ªã l·ªói ( th·ªß c√¥ng)
# 4.Proxy v√† cookies: ch∆∞a t·∫°o ƒë∆∞·ª£c v√¨ ko c√≥ proxy free cho giao th·ª©c https
# 5.Clean Description
# 6.X·ª≠ l√Ω trong 1 file kh√¥ng b·ªã tr√πng product ID
# 7.Ch·∫°y t·ª± ƒë·ªông khi b·ªã l·ªói
# ========= CONFIG ==========
EXCEL_FILE = r"../input/products_id.xlsx"
SHEET_NAME = 'goc'
COLUMN_ID = 'id'
LINES_PER_FILE = 1000
API_URL_TEMPLATE = "https://api.tiki.vn/product-detail/api/v1/products/{}"
OUTPUT_FOLDER = "../output"
LOG_FOLDER = "../logs"
CHECKPOINT_FILE = os.path.join(OUTPUT_FOLDER, "checkpoint.json")
path_fail_file = os.path.join(LOG_FOLDER, "failed_ids.log")
INDEX_FILE = os.path.join(OUTPUT_FOLDER, "file_index.txt")
BATCH_SIZE = 100
SEM_LIMIT = 50
# ===========================


os.makedirs(OUTPUT_FOLDER, exist_ok=True)
os.makedirs(LOG_FOLDER, exist_ok=True)

headers = {
    'User-Agent': 'Mozilla/5.0',
    'Accept': 'application/json; charset=utf-8'
}

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
async def fetch_product_data(client, product_id, sem):
    url = API_URL_TEMPLATE.format(product_id)
    async with sem:
        try:
            response = await client.get(url, timeout=5)
            if response.status_code == 200:
                try:
                    data = response.json()
                except Exception as e:
                    print(f"‚ùå Kh√¥ng th·ªÉ parse JSON cho ID {product_id}: {e}")
                    print(f"‚Ü™ N·ªôi dung tr·∫£ v·ªÅ: {response.text[:200]}")
                    raise

                print(f"‚úÖ {product_id}")
                return {
                    "id": data.get("id"),
                    "name": data.get("name"),
                    "price": data.get("price"),
                    "url_key": data.get("url_key"),
                    "description": clean_description(data.get("description")),
                    "images": data.get("images")
                }
            else:
                print(f"‚ö†Ô∏è L·ªói {response.status_code} ‚Äì ID: {product_id}")
                raise Exception(f"HTTP {response.status_code}")
        except Exception as e:
            print(f"‚ùå L·ªói ID {product_id}: {e}")
            raise


def write_json(data, path):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

async def save_to_file(data, file_index):
    today_str = datetime.now().strftime('%Y-%m-%d')
    file_path = os.path.join(OUTPUT_FOLDER, f"data_{today_str}_{file_index:03d}.json")
    await asyncio.to_thread(write_json, data, file_path)
    print(f"üíæ ƒê√£ l∆∞u {len(data)} d√≤ng v√†o {file_path}")

def save_to_file_index(file_index):
    with open(INDEX_FILE, "w") as f:
        f.write(str(file_index))

def load_file_index():
    if os.path.exists(INDEX_FILE):
        with open(INDEX_FILE, 'r') as f:
            return int(f.read().strip())
    return 1

def save_checkpoint(done_ids):
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(list(done_ids), f)

def load_checkpoint():
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            return set(json.load(f))
    return set()
def load_failed_ids():
    if os.path.exists(path_fail_file):
        with open(path_fail_file, 'r') as f:
            return set(json.load(f))
    return set()


def clean_description(raw_html):
    soup = BeautifulSoup(raw_html or "", "html.parser")
    for ul in soup.find_all('ul'):
        new_ul = soup.new_tag('ul')
        for p in ul.find_all('p'):
            li = soup.new_tag('li')
            li.string = p.get_text(strip=True)
            new_ul.append(li)
        ul.replace_with(new_ul)

    for tag in soup.find_all(['span', 'strong']):
        tag.unwrap()

    for tag in soup.find_all():
        if tag.name not in ['p', 'ul', 'li', 'br', 'img'] and not tag.get_text(strip=True):
            tag.decompose()

    clean_text = re.sub(r'\s+', ' ', soup.prettify())
    return clean_text.strip()

async def main():
    df = pd.read_excel(EXCEL_FILE, sheet_name=SHEET_NAME)
    ids = df[COLUMN_ID].dropna().astype(str).tolist()
    done_ids = load_checkpoint()
    remaining_ids = [pid for pid in ids if pid not in done_ids]


    so_phan_tu = len(ids)

    sem = asyncio.Semaphore(SEM_LIMIT)
    batch_data = []
    existing_ids = set()
    file_index = load_file_index() # n·∫øu b·ªã d·ª´ng v√† ch·∫°y l·∫°i th√¨ ƒë√°nh s·ªë file ti·∫øp t·ª•c, kh√¥ng ch·∫°y ƒë√® l√™n file c≈©
    failed_ids = []

    async with httpx.AsyncClient(headers=headers) as client:
        for chunk in chunked(remaining_ids, BATCH_SIZE):
            print(f"üöÄ ƒêang x·ª≠ l√Ω batch {chunk[0]} ƒë·∫øn {chunk[-1]}")
            tasks = [fetch_product_data(client, pid, sem) for pid in chunk]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for i, res in enumerate(results):
                pid = chunk[i]
                if isinstance(res, dict):
                    if res["id"] not in existing_ids:
                        batch_data.append(res)
                        existing_ids.add(res["id"])
                        done_ids.add(pid)

                    if len(batch_data) >= LINES_PER_FILE:
                        await save_to_file(batch_data, file_index)
                        file_index += 1
                        save_checkpoint(done_ids)
                        save_to_file_index(file_index)
                        batch_data = []
                        existing_ids = set()
                else:
                    failed_ids.append(pid)

        # L∆∞u ph·∫ßn c√≤n l·∫°i 1
        if batch_data:
            await save_to_file(batch_data, file_index)
            save_checkpoint(done_ids)
            save_to_file_index(file_index)

        # Ghi l·ªói l·∫ßn 1
        if failed_ids:
            log_path = os.path.join(LOG_FOLDER, "failed_ids.log")
            with open(log_path, 'w', encoding='utf-8') as f:
                   json.dump(list(failed_ids), f)


        fail_ids = load_failed_ids()
        retry_fail_ids = [pid for pid in fail_ids]
        print(retry_fail_ids)
        failed_ids.clear()
        # l·∫ßn 2 ch·∫°y tr√™n file log
        async with httpx.AsyncClient(headers=headers) as client:
            for chunk in chunked(retry_fail_ids, BATCH_SIZE):
                print(f"üöÄ ƒêang x·ª≠ l√Ω batch {chunk[0]} ƒë·∫øn {chunk[-1]}")
                tasks = [fetch_product_data(client, pid, sem) for pid in chunk]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                for i, res in enumerate(results):
                    pid = chunk[i]
                    if isinstance(res, dict):
                        if res["id"] not in existing_ids:
                            batch_data.append(res)
                            existing_ids.add(res["id"])
                            done_ids.add(pid)

                        if len(batch_data) >= LINES_PER_FILE:
                            await save_to_file(batch_data, file_index)
                            file_index += 1
                            save_checkpoint(done_ids)
                            save_to_file_index(file_index)
                            batch_data = []
                            existing_ids = set()
                    else:
                        failed_ids.append(pid)

            # L∆∞u ph·∫ßn c√≤n l·∫°i 2
            if batch_data:
                await save_to_file(batch_data, file_index)
                save_checkpoint(done_ids)
                save_to_file_index(file_index)
                # Ghi l·ªói l·∫ßn 2
            if failed_ids:
                log_path = os.path.join(LOG_FOLDER, "failed_ids.log")
                with open(log_path, 'w', encoding='utf-8') as f:
                    for pid in failed_ids:
                        f.write(f"{pid}\n")
                print(f"üìÑ Ghi {len(failed_ids)} ID l·ªói v√†o {log_path}")
    def restart_script():
        print("üîÅ ƒêang kh·ªüi ƒë·ªông l·∫°i script...")
        time.sleep(5)
        os.execv(executable, ['python'] + argv)

    total_success = len(done_ids)
    print(f"\n‚úÖ T·ªïng th√†nh c√¥ng: {total_success}")
    print(f"‚ùå T·ªïng l·ªói: {so_phan_tu - total_success}")

if __name__ == "__main__":

        try:
            start1 = datetime.now()
            asyncio.run(main())
            end1 = datetime.now()
            print(f"B·∫Øt ƒë·∫ßu l√∫c: {start1}")
            print(f"Ho√†n th√†nh sau: {end1 - start1}")
        except Exception as e:
            print(f"‚ùå Ch∆∞∆°ng tr√¨nh l·ªói: {e}")

